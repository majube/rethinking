---
title: "Notebook Exercises Chapter 11"
author: "Max Beauchez"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
library(rethinking)
```

**11E1.** If an event has probability 0.35, what are the log-odds of this event?

The log-odds are given by $log\,{p \over {1-p}}$.

For $p = 0.35$:

```{r}
p <- 0.35
log(p/(1 - p))
```


**11E2.** If an event has log-odds 3.2, what is the probability of this event?

Rewriting the relation given in **11E1**, with $\alpha$ the log-odds:

$\alpha = log\,{p \over {1-p}}$

$p = {exp(\alpha) \over {1 + exp(\alpha)}}$

In R code:

```{r}
alpha <- 3.2
(p <- exp(alpha) / (1 + exp(alpha)))
```


**11E3.** Suppose that a coefficient in a logistic regression has value 1.7. What does this imply about the proportional change in odds of the outcome?

It implies that the proportional odds of the outcome change by a factor of $exp(1.7) =$ `r exp(1.7)` for each unit change in the predictor variable.


**11E4.** Why do Poisson regressions sometimes require the use of an *offset*? Provide an example.

Because of different exposure times for the counts being modeled. This is modeled as:

$y_{i} \sim Poisson(\lambda_{i})$

$\lambda_{i} = {\mu_{i} \over \tau_{i}} = exp(\alpha + \beta x_{i})$, with $\tau_{i}$  the exposure.

Rewriting, we get:

$y_{i} \sim Poisson(\mu_{i})$

$log\,\mu_{i} = log\,\tau_{i} + alpha + \beta x_{i}$

An example might be traffic counts at certain points on roads. One measurement post might report this data on an hourly basis, whereas a measurement posts on a more quiet road might report this on a daily basis.


**11M1.** As explained in the chapter, binomial data can be organized in aggregated and disaggregated forms, without any impact on inference. But the likelihood of the data does change when the data are converted between the two formats. Can you explain why?

With the data in aggregated form the multiplicity needs to be accounted for in the likelihood: the number of ways in which we could get the observed number of counts.

For example, for an event with probability $p$ that occurs once in three trials, the likelihood is:

${3! \over 1!(3 - 1)!}\,p^1(1-p)^2=3\times p^1(1-p)^2$

The factor three at the beginning reflects the fact that the event can happen in the first, second or third trial.


**11M2.** If a coefficient in a Poisson regression has value 1.7, what does this imply about the change in the outcome?

With this information we can only calculate the proportional odds by exponentiating the parameter: $exp(1.7)$ = `r exp(1.7)`; for one unit of change in the predictor variable associated with this parameter we can expect a change in the odds by a factor of `r exp(1.7)`. 


**11M3.** Explain why the logit link is appropriate for a binomial generalized linear model.

It constrains the probability to be positive and between 0 and 1, and ensures that increasingly extreme values of the predictor variable have diminishing effects on the probability.


**11M4.** Explain why the log link is appropriate for a Poisson generalized linear model.

The rate parameter of the Poisson distribution needs to be positive; the log link function ensures that this is the case.


**11M5.** What would it imply to use a logit link for the mean of a Poisson generalized linear model? Can you think of a real research problem for which this would make sense?

It would constrain the parameter $\lambda$, which is both the expected value and variance of the distribution, to be in the range $[0, 1]$. It doesn't make a lot of sense for a Poisson GLM, as these are normally used to model counts without a known maximum value.

If you add a non-standard scaling factor $M$ representing the maximum count to the logit link function, so that it would be:

$y_{i} \sim Poisson(\lambda_{i})$

$log\,\lambda_{i} = M {exp(\alpha + \beta x_{i}) \over {1 + exp(\alpha + \beta x_{i})}}$

Then $\lambda_{i}$ would asymptotically approach this value of M for very large value of the predictor variable $x_{i}$ (assuming positive $\beta$). This could be a sensible option to use if there is a certain known maximum value for either the expected value or the variance of the distribution of counts.


**11M6.** State the constraints for which the binomial and Poisson distributions have maximum entropy. Are the constraints different at all for binomial and Poisson? Why or why not?

  * Only two unordered outcomes are possible
  
  * The expected probabilities of each type of event are assumed to be constant
  
The constraints for the binomial and Poisson distributions are the same, because the Poisson distribution is a special case of the binomial distribution (with a very large number of trials and a very low event probability).


**11M7.** Use `quap` to construct a quadratic approximate posterior distribution for the chimpanzee model that includes a unique intercept for each actor, `m11.4` (page 330). Compare the quadratic approximation to the posterior distribution produced instead from MCMC. Can you explain both the differences and the similarities between the approximate and the MCMC distributions? Relax the prior on the actor intercepts to Normal(0, 10). Re-estimate the posterior using both `ulam` and `quap`. Do the differences increase or decrease? Why?

```{r warning=FALSE, message=FALSE}
data(chimpanzees)
d <- chimpanzees
d$treatment <- as.integer(1 + d$prosoc_left + 2*d$condition)
dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = as.integer(d$treatment)
)

m_quap <- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
 ), data=d)

m_ulam <- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
 ), data=dat_list, chains=4, cores=4)

plot(coeftab(m_quap, m_ulam))
```

The estimates are almost exactly the same; only for `a[2]` a small difference is visible. Inspecting the posterior of `a[2]` specifically:

```{r}
post_quap <- extract.samples(m_quap)
post_ulam <- extract.samples(m_ulam)
dens(post_quap$a[, 2])
dens(post_ulam$a[, 2], lt="dashed", add=TRUE)
```

We can see that the shape of the posterior estimated using MCMC with `ulam` is skewed away from zero, whereas `quap` of course estimated a symmetric Gaussian.

Widening the prior on `a`:

```{r warning=FALSE, message=FALSE}
m_quap_wide_prior <- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 10),
    b[treatment] ~ dnorm(0, 0.5)
 ), data=d)

m_ulam_wide_prior <- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 10),
    b[treatment] ~ dnorm(0, 0.5)
 ), data=dat_list, chains=4, cores=4)

plot(coeftab(m_quap_wide_prior, m_ulam_wide_prior))
```

The difference between MCMC (`ulam`) and quadratic approximation (`quap`) is now more obvious: the MCMC `ulam` estimate is much further away from zero than the `quap` one. 

```{r}
post_quap_wide_prior <- extract.samples(m_quap_wide_prior)
post_ulam_wide_prior <- extract.samples(m_ulam_wide_prior)
dens(post_quap_wide_prior$a[, 2])
dens(post_ulam_wide_prior$a[, 2], lt="dashed", add=TRUE)
```

We can see here that the assumption of a Gaussian posterior used in `quap` doesn't hold, and that the shape of the posterior is actually quite different. 


**11M8.** Revisit the `data(Kline)` islands example. This time drop Hawaii from the sample and refit the models. What changes do you observe?

```{r warning=FALSE, message=FALSE}
data(Kline)
d <- Kline
d$contact_id <- ifelse(d$contact == "high", 2, 1)
d_wo_hawaii <- d[-which(d$culture == "Hawaii"), ]
d$P <- scale(log(d$population))
d_wo_hawaii$P <- scale(log(d_wo_hawaii$population))
d_slim <- list(T=d$total_tools, cid=d$contact_id, P=d$P)
d_wo_hawaii_slim <- list(T=d_wo_hawaii$total_tools, cid=d_wo_hawaii$contact_id, P=d_wo_hawaii$P)

mdef_intercept <- alist(
    T ~ dpois(lambda),
    log(lambda) <- a,
    a ~ dnorm(3, 0.5)
 )
mdef_interaction <- alist(
    T ~ dpois(lambda),
    log(lambda) <- a[cid] + b[cid]*P,
    a[cid] ~ dnorm(3, 0.5),
    b[cid] ~ dnorm(0, 0.2)
 )

m_intercept <- ulam(mdef_intercept, data=d_slim, chains=4, cores=4, log_lik=TRUE)
m_interaction <- ulam(mdef_interaction, data=d_slim, chains=4, cores=4, log_lik=TRUE)
m_intercept_wo_hawaii <- ulam(mdef_intercept, data=d_wo_hawaii_slim, chains=4, cores=4, log_lik=TRUE)
m_interaction_wo_hawaii <- ulam(mdef_interaction, data=d_wo_hawaii_slim, chains=4, cores=4, log_lik=TRUE)
```

```{r}
coeftab(m_intercept, m_intercept_wo_hawaii)
```
As was to be expected, the intercept-only model has a lower intercept (average) without Hawaii, which has the largest number of tools in the data set.


```{r}
coeftab(m_interaction, m_interaction_wo_hawaii)
```

With the interaction model we see that the estimated intercepts are lower (corresponding to a lower average without Hawaii), and that the estimated slopes are almost the same value now, meaning there is little difference in the expected number of tools between high- and low-contact societies with the same populations.


**11H1.** Use WAIC or PSIS to compare the chimpanzee model that includes a unique intercept for each actor, `m11.4` (page 330), to the simpler models fit in the same section. Interpret the results.

```{r warning=FALSE, message=FALSE}
data(chimpanzees)
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition

m11.1 <- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a,
    a ~ dnorm(0, 10)
 ), data=d)

m11.2 <- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a + b[treatment],
    a ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 10)
 ), data=d)

m11.3 <- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a + b[treatment],
    a ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
 ), data=d)

dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = as.integer(d$treatment)
)

m11.4 <- ulam(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + b[treatment],
    a[actor] ~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5)
 ), data=dat_list, chains=4, cores=4, log_lik=TRUE)

compare(m11.1, m11.2, m11.3, m11.4, func=PSIS)
```

`m11.4`, the model with an individual intercept for each actor has significantly better predictive accuracy than the other models. This is in line with the conclusion that the treatment has little to no effect, and that the handedness preference of each chimpanzee is much more influential.


**11H2.** The data contained in `library(MASS); data(eagles)` are records of salmon pirating attempts by Bald Eagles in Washington State. See `?eagles` for details. While on eagle feeds, sometimes another will swoop in and try to steal the salmon from it. Call the feeding eagle the "victim" and the thief the "pirate". Use the available data to build a binomial GLM of successful pirating attempts.

  a. Consider the following model:
  
  $y_{i} \sim Binomial(n_{i}, p_{i})$
  
  $logit\,(p_{i}) = \alpha + \beta_{P}P_{i} + \beta_{V}V_{i} + \beta_{A}A_{i}$
  
  $\alpha \sim Normal(0, 1.5)$
  
  $\beta_{P}, \beta_{V}, \beta_{A} \sim Normal(0, 0.5)$
  
  where y is the number of successful attempts, $n$ is the total number of attempts, $P$ is a dummy variable indicating whether or not the pirate had large body size, $V$ is a dummy variable indicating whether or not the victim had large body size, and finally $A$ is a dummy variable indicating whether or not the pirate was an adult. Fit the model above to the `eagles` data, using both `quap` and `ulam`. Is the quadratic approximation okay?
  
  b. Now interpret the estimates. If the quadratic approximation turned out okay, then it's okay to use the `quap` estimates. Otherwise stick to `ulam` estimates. Then plot the posterior predictions. Compute and display both (1) the predicted **probability** of success and its 89% interval for each row ($i$) and in the data, as well as (2) the predicted success **count** and its 89% interval. What different information does each type of posterior prediction provide?
  
  c. Now try to improve the model. Consider an interaction between the pirate's size and age (immature or adult). Compare this model to the previous one, using WAIC. Interpret.
  
a. Specifying the model and estimating it using both `quap` and `ulam`, and inspecting the posterior estimates.

```{r warning=FALSE, message=FALSE}
library(MASS)
data(eagles)
d <- eagles
d$P <- ifelse(d$P == "S", 0, 1)
d$V <- ifelse(d$P == "S", 0, 1)
d$A <- ifelse(d$A == "I", 0, 1)

mH2_quap <- quap(
  alist(
    y ~ dbinom(n, p),
    logit(p) <- a + bP*P + bV*V + bA*A,
    a ~ dnorm(0, 1.5),
    c(bP, bV, bA) ~ dnorm(0, 0.5)
  ), data=d
)

mH2_ulam <- ulam(
  alist(
    y ~ dbinom(n, p),
    logit(p) <- a + bP*P + bV*V + bA*A,
    a ~ dnorm(0, 1.5),
    c(bP, bV, bA) ~ dnorm(0, 0.5)
  ), data=d, chains=4, cores=4, log_lik=TRUE
)
tryCatch(plot(coeftab(mH2_quap, mH2_ulam)), error = function(e) coeftab(mH2_quap, mH2_ulam))
```




**11H3.** The data contained in `data(salamanders)` are counts of salamanders (*Plethodon elongatus*) from 47 different 49-m^2 plots in northern California. The column SALAMAN is the count in each plot, and the columns PCTCOVER and FORESTAGE are percent of ground cover and age of trees in the plot, respectively. Uou will model SALAMAN as a Poisson variable.

  a. Model the relationship between density and percent cover, using a log-link same as the example in the book and lecture). Use weakly informative priors of your choosing. Check the quadratic approximation again, by comparing `quap` to `ulam`. Then plot the expected counts and their 89% interval against percent cover. In which ways does the model do a good job? A bad job?
  
  b. Can you improve the model by using the other predictor, FORESTAGE? Try any models you think useful. Can you explain why FORESTAGE helps or does not help with prediction?


**11H4.** The data in `data(NWOGrants)` are outcomes for scientific funding application for the Netherlands Oranization for Scientific Research (NWO) from 2010-2012 (see van der Lee and Ellemers (2015 for data and context), These data have a very similar structure to the `UCBAdmit` data discussed in the chapter. I want you to consider a similar question: What are the total and indirect causal effects of gender on grant awards? Consider a mediation path (a pipe) through `discipline`. Draw the corresponding DAG and then use one or more binomial GLMs to answer the question. What is your causal interpretation? If NWO's goal is to equalize rates of funding between men and women, what type of intervention would be most effective?


**11H5.** Suppose that the NWO Grants sample has an unobserved confound that influences both choice of discipline and the probability of a an award. One example of such a confound could be the career stage of each applicant. Suppose that in some disciplines, junior scholars apply for most of the grants. In other disciplines, scholars from all career stages compete. As a result, career stage influences discipline as well as the probability of being awarded a grant. Add these influences to your DAG from the previous problem. What happens now when you condition on discipline? Does it provide an un-confounded estimate of the direct path from gender to an award? Why or why not? Justify your answer with the backdoor criterion. If you have trouble thinking this through, try simulating fake data, assuming your DAG is true. Then analyze is using the model from the previous problem. What do you conclude? Is it possible for gender to have a real direct causal influence but for a regression conditioning on both gender and discipline to suggest zero influence?


**11H6.** The data in `data(Primates301)` are 301 primate species and associated measures. In this problem, you will consider how brain size is associated with social learning. There are three parts.

  a. Model the number of observations of `social_learning` for each species as a function of the log `brain` size. Use a Poisson distribution for the `social_learning` outcome variable. Interpret the resulting posterior.
  
  b. Some species are studied much more than others. So the number of reported instances of `social_learning` could be a product of research effort. Use the `research_effort` variable, specifically its logarithm, as an additional predictor variable. Interpret teh coefficient for log `research_effort`. How does this model differ from the previous one?
  
  c. Draw a DAG to represent how you think the variables `social_learning`, `brain` and `research_effort` interact. Justify the DAG with the measured associations in the two models above (and any other models you used).

