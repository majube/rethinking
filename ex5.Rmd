---
title: "Notebook Exercises Chapter 5"
author: "Max Beauchez"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
```

**5E1.** Which of the linear models below are multiple linear regression?

  1. $\mu_{i} = \alpha + \beta x_{i}$
  
  2. $\mu_{i} = \beta_{x} x_{i} + \beta_z z_{i}$
  
  3. $\mu_{i} = \alpha + \beta(x_{i} - z_{i})$
  
  4. $\mu_{i} = \alpha + \beta_{x} x_{i} + \beta_{z} z_{i}$
  
Options 2 and 4 are multiple linear regression: they include multiple predictor variables with their own parameters in the equation for $\mu_{i}$. Option 3 has multiple predictor variables but only uses the difference between them as a predictor variable.


**5E2.** Write down a multiple regression to evaluate the claim: *Animal diversity is linearly related to altitude, but only after controlling for plant diversity*. You just need to write down the model definition.

With animal diversity as $D$, altitude as $A$ and plant diversity as $P$:

$D_{i} \sim Normal(\mu_{i}, \sigma)$

$\mu_{i} = \alpha + \beta_{A}A + \beta_{P}P$


**5E3.** Write down a multiple regression to evaluate the claim: *Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree*. Write down the model definition and indicate with side of zero each slope parameter should be on.

With $T$ time to PhD degree, $L$ laboratory size and $F$ funding:

$T_{i} \sim Normal(\mu_{i}, \sigma)$

$\mu_{i} = \alpha + \beta_{L}L + \beta_{F}F$

Both $\beta_{L}$ and $\beta_{F}$ should be positive (>0). For neither variable to be a good predictor of time to PhD by itself, the variables have to be negatively correlated; that is large labs tend to have little funding, and vice versa.

**5E4.** Suppose you have a single categorical predictor with 4 levels (unique values), labeled A, B, C and D. Let $A_{i}$ be an indicator variable that is 1 where case $i$ is in category $A$. Also suppose $B_{i}$, $C_{i}$, and $D_{i}$ for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when it's possible to compute one posterior distribution from the posterior distribution of another model.

  1. $\mu_{i} = \alpha + \beta_{A}A_{i} + \beta_{B}B_{i} + \beta_{D}D_{i}$
  
  2. $\mu_{i} = \alpha + \beta_{A}A_{i} + \beta_{B}B_{i} + \beta_{C}C_{i} + \beta_{D}D_{i}$
  
  3. $\mu_{i} = \alpha + \beta_{B}B_{i} + \beta_{C}C_{i} + \beta_{D}D_{i}$
  
  4. $\mu_{i} = \alpha_{A}A_{i} + \alpha_{B}B_{i} + \alpha_{C}C_{i} + \alpha_{D}D_{i}$
  
  5. $\mu_{i} = \alpha_{A}(1 - B_{i} - C_{i} - D_{i}) + \alpha_{B}B_{i} + \alpha_{C}C_{i} + \alpha_{D}D_{i}$
  
Options 1 and 3 use an intercept and indicator variables, and are inferentially equivalent; $\alpha$ just encodes a different 'default' value (C and A, respectively). Options 4 and 5 use index variables and are inferentially equivalent to each other, as $A_{i} = (1 - B_{i} - C_{i} - D_{i})$.

**5M1.** Invent your own example of a spurious correlation. An outcome variable should be correlated with both predictor variables. But when both predictors are entered in the same model, the correlation between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).

```{r}
N <- 1e3
x1 <- rnorm(N, mean=20)
x2 <- rnorm(N, mean=-2*x1)
y <- rnorm(N, mean=3*x1)
#bivariate
m_x1 <- quap(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- b1*x1,
    b1 ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data=list(x1=x1)
)
m_x2 <- quap(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- b2*x2,
    b2 ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data=list(x2=x2)
)
#multivariate
m_mv <- quap(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- b1*x1 + b2*x2,
    b1 ~ dnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data=list(x1=x1, x2=x2)
)
#sometimes doesn't work interactively for some reason, but works in knitr, see https://github.com/rmcelreath/rethinking/issues/22
plot(coeftab(m_x1, m_x2, m_mv), pars=c("b1", "b2"))
```

The coefficient for `b2` for variable `x2` is non-zero in the bivariate model, but zero in the multivariate model, because `x2` is a direct function of `x1`. `y` is also a direct function of `x1`, so once it is known, there is no additional value in knowing `x2`.


**5M2.** Invent your own example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another.

```{r}
N <- 1e3
x1 <- rnorm(N)
x2 <- rnorm(N, mean=-x1)
y <- rnorm(N, mean=3*x1+3*x2)
plot(x1, x2)
#bivariate
m_x1 <- quap(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- b1*x1,
    b1 ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data=list(x1=x1)
)
m_x2 <- quap(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- b2*x2,
    b2 ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data=list(x2=x2)
)
#multivariate
m_mv <- quap(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- b1*x1 + b2*x2,
    b1 ~ dnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ), data=list(x1=x1, x2=x2)
)
#sometimes doesn't work interactively for some reason, but works in knitr, see https://github.com/rmcelreath/rethinking/issues/22
plot(coeftab(m_x1, m_x2, m_mv), pars=c("b1", "b2"))
```


**5M3.** It is sometimes observed that the best predictor of fire risk is the presence of firefightersâ€”States and localities with many firefighters also have more fires. Presumably firefighters do not *cause* fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the same reversal of causal inference in the context of the divorce and marriage data. How might a high divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using multiple regression?

A higher divorce rate would be expected to lead to a higher equilibrium size of the single population available for marriage, a certain fraction of which gets married, so also a higher marriage rate.

A multiple regression that includes as a predictor variable the share of non-first marriages can help to evaluate this relationship.

**5M4.** In the divorce data, States with high numbers of members of the Church of Jesus Christ of Latter-day Saints (LDS) have much lower divorce rates than the regression model expected. Find a list of LDS population by State and use those numbers as a predictor variable, predicting divorce rate using marriage rate, median age at marriage and percent LDS population. You may want to consider transformations of the raw percent LDS variable.

I downloaded the Mormon population percentage by state [from Wikipedia](https://en.wikipedia.org/wiki/The_Church_of_Jesus_Christ_of_Latter-day_Saints_membership_statistics_(United_States)) on 25/10/2022 and saved it in `lds.csv`.

```{r}
data("WaffleDivorce")
lds <- read.csv("lds.csv")
d <- WaffleDivorce
#standardize
d$D <- standardize(d$Divorce)
d$M <- standardize(d$Marriage)
d$A <- standardize(d$MedianAgeMarriage)
d$L <- standardize(lds$pct)
m_divorce <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bM*M + bA*A + bL*L,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    bA ~ dnorm(0, 0.5),
    bL ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)
precis(m_divorce)
```

The posterior mean for `bL`, the effect of the percentage of Mormons of the population is reliably negative, with the 89% probability interval being [-0.5, -0.12], and an average of -0.31. That is to say, an increase of the percentage of the Mormon population by one standard deviation of `r round(sd(lds$pct), 2)`% is expected to lead to a reduction in the marriage rate of `r round(precis(m_divorce)$mean[4]*sd(d$Divorce), 2)`%.


**5M5.** One way to reason through multiple causation hypotheses is to imagine detailed mechanisms through which predictor variables may influence outcomes. For example, it is sometimes argued that the price of gasoline (predictor variable) is positively associated with lower obesity rates (outcome variable). However, there are at least two important mechanisms by which the price of gas could reduce obesity. First, it could lead to less driving and therefore more exercise. Second, it could lead to less driving, which leads to less eating out, which leads to less consumption of huge restaurant meals. Can you outline one or more multiple regressions that address these two mechanisms? Assume you can have any predictor data you need.

Let G denote gas prices, D driving, E exercise, O obesity and R eating out (for restaurant). Below is a DAG that incorporates both causal paths.

```{r}
library(dagitty)
dag <- dagitty("dag{G -> D -> E -> O; D -> R -> O}")
coordinates(dag) <- list(x=c(R=0, D=0.5, O=0.5, G=0.5, E=1), y=c(O=1.5, E=1, R=1, D=0.5, G=0))
drawdag(dag)
```

If, as stipulated, we can have any predictor data we need, then a multiple regression of O on R and E would be able to ascertain the relative magnitude of these two causal paths. The data for R could be restaurant turnover and average steps walked for E, both for different regions.

**5H1.** In the divorce example, suppose the DAG is $M \to A \to D$. What are the implied conditional independencies of the graph? Are the data consistent with it?

The implied conditional independencies are:

```{r}
dag <- dagitty("dag{M -> A -> D}")
impliedConditionalIndependencies(dag)
```

That is, D is independent of M conditional on A. Is this consistent with the data?

```{r}
mH1 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bM*M + bA*A,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)
plot(coeftab(mH1), pars=c("bM", "bA"))
```

It does seem to be the case that the effect of M on D when given A is around 0 (bM around 0).


**5H2.** Assuming that the DAG for the divorce example is indeed $M \to A \to D$, fit a new model and use it to estimate the counterfactual effect of halving a State's marriage rate *M*. Use the counterfactual example from the chapter (starting on page 140) as a template.

```{r}
mH2 <- quap(
  alist(
    ## M -> A
    A ~ dnorm(mu_A, sigma_A),
    mu_A <- bM*M,
    bM ~ dnorm(0, 0.5),
    sigma_A ~ dexp(1),
    ## A -> D
    D ~ dnorm(mu_D, sigma_D),
    mu_D <- bA*A,
    bA ~ dnorm(0, 0.5),
    sigma_D ~ dexp(1)
  ), data=d
)
precis(mH2)
coefs <- coef(mH2)
coefs[["bM"]] * coefs[["bA"]]
```
A decrease in a state's marriage rate by 1 standard deviation would lead to a decrease in the divorce rate by `r round(coefs[["bM"]] * coefs[["bA"]], 2)` standard deviation. The average state has a marriage rate of `r round(mean(d$Marriage), 2)`, with a standard deviation of `r round(sd(d$Marriage), 2)`. Halving the average marriage rate to `r round(mean(d$Marriage)/2, 2)` would represent a decrease of `r round(0.5*mean(d$Marriage)/sd(d$Marriage), 2)` standard deviations, which would lead to an expected decrease in the divorce rate of `bM * bA * -2.65 = 0.39 * -2.65 = ``r round(-0.5*mean(d$Marriage)/sd(d$Marriage)*coefs[["bM"]]*coefs[["bA"]], 2)`


**5H3.** Return to the milk energy model, `m5.7`. Suppose that the true causal relationship among the variables is:
```{r echo=FALSE}
dag <- dagitty("dag{M -> N; N -> K; M -> K}")
coordinates(dag) <- list(x=c(M=0, K=0.5, N=1), y=c(M=0, N=0, K=1)) 
drawdag(dag)
```

Now compute the counterfactual effect on *K* of doubling *M*. You will need to account for both the direct and indirect paths of causation. Use the counterfactual example from the chapter (page 140) as a template.

```{r}
data(milk)
d <- milk
d$K <- standardize(d$kcal.per.g)
d$N <- standardize(d$neocortex.perc)
d$M <- standardize(log(d$mass))
dcc <- d[complete.cases(d$K, d$N, d$M), ]
mH3 <- quap(
  alist(
    ## M -> K <- N
    K ~ dnorm( mu , sigma ),
    mu <- a + bM*M + bN*N,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    bN ~ dnorm(0, 0.5),
    sigma ~ dexp(1),
    ## M -> N
    N ~ dnorm( mu_N , sigma_N),
    mu_N <- aN + bMN*M,
    aN ~ dnorm(0, 0.2),
    bMN ~ dnorm(0, 0.5),
    sigma_N ~ dexp(1)
  ), data = dcc)
precis(mH3)
```

```{r}
M_seq <- (seq(from=log(0.5), to=log(80), length.out=100) - mean(log(d$mass)))/sd(log(d$mass))
sim_dat <- data.frame(M=M_seq)
s <- sim(mH3, data=sim_dat, vars=c("N", "K"))
plot(sim_dat$M, colMeans(s$K), xlim=range(M_seq), ylim=c(-2, 2), type="l", xlab="Manipulated M", ylab="Counterfactual K")
shade(apply(s$K, 2, PI, prob=0.89), sim_dat$M)
mtext("Total counterfactual effect of M on K")
```


**5H4.** Here is an open practice problem to engage your imagination. In the divorce data, States in the southern United States have many of the highest divorce rates. Add the `South` indicator variable to the analysis. First, draw one or more DAGs that represent your ideas for how Southern American culture might influence any of the three other variables (*D*, *M* or *A*). Then list the testable implications of your DAGs, if there are any, and fit one or more models to evaluate the implications. What do you think the influence of "Southerness" is?

#TODO